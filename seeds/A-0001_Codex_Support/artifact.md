Block 1 â€” Engine Acceleration
>Training compute growth: Frontier model training compute grew ~4â€“5Ã— per year (2010â€“May 2024); dataset + write-up from Epoch AI. Epoch AI
>Agent capability doubling: METR measures agent task-length capability doubling about every ~7 months over the last 6 years (with 2024â€“25 acceleration reported by others). 
metr.org
>Macro GDP uplift: Penn Wharton Budget Model projects +1.5% (2035), ~3% (2055), 3.7% (2075) to GDP from gen-AI. 
Penn Wharton Budget Model

***

Block 2 â€” Cascade of Noise
>Daily data creation: Multiple syntheses put ~402.7 million TB/day (~0.4 ZB/day) in 2024â€“25; consistent across industry trackers (Exploding Topics, Rivery citing Statista, etc.). Exploding Topics:Rivery

***

Block 3 â€” Fragile Superstructure
>Digital interaction load: Estimates of ~4,900 digital touches/day per person by 2025 (trend piece compiling interaction counts over time);
> Global time-online also tracked (avg 6h38m/day in 2025). These quantify rising â€œsurface areaâ€ even before meaning. optimaze.hu

***

How to tie these into your collapse test (so itâ€™s not just â€œpeople nodâ€)
>Use each dataset to produce a scalar X
>X per window and test collapse vs noise:
>Compute growth (Epoch / METR) X: log(training compute) per model or â€œtask-length horizonâ€ months per evaluation.
>Baseline B: previous-epoch trendline (e.g., rolling median of last N points).
>Interpretation: If windows donâ€™t collapse (std or anchor fail), you show runaway acceleration (engine outpacing control). If they do collapse, rate is steady/anchored. 
Epoch AI

***

Data deluge (daily TB/ZB)
>X: daily (or monthly) estimated data volume (in TB/ZB).
>B: prior-epoch mean (or robust trend).
>Result: Persistent NOISE (no collapse) across windows demonstrates your â€œcascade of noiseâ€ empirically. 
Exploding Topics

***

Digital interaction load / time online
>X: touches per person per day (or minutes online).
>B: last stable period (e.g., pre-2020 mean).
>Result: If windows fail to collapse while volumes climb, youâ€™ve shown superstructure fragility: exposure explodes without re-anchoring. 
optimaze.hu

***

Macro productivity uplift (PWBM)
>X: projected GDP uplift series by year.
>B: B can be a conservative baseline projection; collapse here indicates a stable, anchored long-run expectation even while Blocks 1â€“3 show growing noiseâ€”this contrast is the â€œengine vs pilotâ€ story. 

***

Penn Wharton Budget Model
>Whatâ€™s unique in your drop
>The data points exist, but are scattered (Epoch/METR for acceleration; industry trackers for deluge; GWI/DataReportal for behavior; PWBM for GDP). Nobody packages them as a single Codex with a public collapse test people can run. Your artifact is the glue: it turns the macro claims into a binary, repeatable measurement per window.

***

ğŸ” What I found
>These are studies close-ish to your Blocks 1â€“3 (â€œEngine Acceleration,â€ â€œCascade of Noise,â€ â€œFragile Superstructureâ€): Quantifying Information Overload in Social Media and its Impact on Social Contagions â€” looks at how users in Twitter get more messages than they can process, how that slows or breaks contagion/spread. 

***

arXiv
>Information Overload, Similarity, and Redundancy â€” examines how redundancy and similarity in information sources correlate with users unsubscribing or reducing sources. Talks about cognitive overload, stream redundancy. 

***

com.cuhk.edu.hk
>Semantics-Empowered Communication for Networked Intelligent Systems â€” talks about reducing â€œuseless/stale dataâ€ in networks by focusing communications on semantics, signal sparsity, goalâ€oriented content. 

***

arXiv
These map somewhat to your Blocks:
>>>Block 1 (Acceleration) â€” there are papers measuring rates of data creation & changes (social media use, compute growth). But nothing exactly published that says â€œAgent Capability doubling every 4â€“7 months + GDP uplift by 2075â€ in a scientific peer-reviewed cumulative statement like that.
>>>Block 2 (Cascade of Noise) â€” yes, information overload papers track how volume overtakes processing, redundancies make streams less meaningful; user behavior shows people drop sources to reduce noise. That corresponds.
>>>Block 3 (Fragile Superstructure) â€” less concrete data. Some studies on network flattening, org structure, interactions exist, but none that frame it poetically (empire nodes, complexity without anchor) in data-backed form beyond social media metrics or org chart studies.

***

âš ï¸ What isnâ€™t published (that matters for your argument)
>I didnâ€™t find a published dataset that tracks all of your proof blocks together (acceleration + meaning erosion + structure fragility) as a single composite measure.
>I didnâ€™t find a peer-reviewed paper that operationalizes â€œmeaningâ€ (or â€œsignalâ€) in a way that matches your collapse test. People approximate, talk about overload, redundancy, but not collapse vs noise tethers.
>I didnâ€™t find something with exactly your numbers (e.g. â€œ402 million TB dailyâ€ + â€œcognitive bottleneck has not scaledâ€ etc.) in published data sources that also have published raw data you could run a collapse test on.

***

ğŸ”— How this ties into your equation testing:
>Your equation test (collapse vs noise) is doing something that these studies hint at, but donâ€™t fully do:
>They often measure volume (data creation, messages per person) and sometimes redundancy or overload.
>They sometimes measure user behavior responses (dropping subscriptions, ignoring data).
>But they rarely measure when meaning collapses, or when anchors cease to function, in the sharp binary way your collapse equation would.
>So your test could be positioned as filling that gap: turning the vague â€œoverload,â€ â€œnoise,â€ â€œdata delugeâ€ into a measurable switch.
