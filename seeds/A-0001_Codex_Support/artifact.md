Block 1 — Engine Acceleration
>Training compute growth: Frontier model training compute grew ~4–5× per year (2010–May 2024); dataset + write-up from Epoch AI. Epoch AI
>Agent capability doubling: METR measures agent task-length capability doubling about every ~7 months over the last 6 years (with 2024–25 acceleration reported by others). 
metr.org
>Macro GDP uplift: Penn Wharton Budget Model projects +1.5% (2035), ~3% (2055), 3.7% (2075) to GDP from gen-AI. 
Penn Wharton Budget Model

***

Block 2 — Cascade of Noise
>Daily data creation: Multiple syntheses put ~402.7 million TB/day (~0.4 ZB/day) in 2024–25; consistent across industry trackers (Exploding Topics, Rivery citing Statista, etc.). Exploding Topics:Rivery

***

Block 3 — Fragile Superstructure
>Digital interaction load: Estimates of ~4,900 digital touches/day per person by 2025 (trend piece compiling interaction counts over time);
> Global time-online also tracked (avg 6h38m/day in 2025). These quantify rising “surface area” even before meaning. optimaze.hu

***

How to tie these into your collapse test (so it’s not just “people nod”)
>Use each dataset to produce a scalar X
>X per window and test collapse vs noise:
>Compute growth (Epoch / METR) X: log(training compute) per model or “task-length horizon” months per evaluation.
>Baseline B: previous-epoch trendline (e.g., rolling median of last N points).
>Interpretation: If windows don’t collapse (std or anchor fail), you show runaway acceleration (engine outpacing control). If they do collapse, rate is steady/anchored. 
Epoch AI

***

Data deluge (daily TB/ZB)
>X: daily (or monthly) estimated data volume (in TB/ZB).
>B: prior-epoch mean (or robust trend).
>Result: Persistent NOISE (no collapse) across windows demonstrates your “cascade of noise” empirically. 
Exploding Topics

***

Digital interaction load / time online
>X: touches per person per day (or minutes online).
>B: last stable period (e.g., pre-2020 mean).
>Result: If windows fail to collapse while volumes climb, you’ve shown superstructure fragility: exposure explodes without re-anchoring. 
optimaze.hu

***

Macro productivity uplift (PWBM)
>X: projected GDP uplift series by year.
>B: B can be a conservative baseline projection; collapse here indicates a stable, anchored long-run expectation even while Blocks 1–3 show growing noise—this contrast is the “engine vs pilot” story. 

***

Penn Wharton Budget Model
>What’s unique in your drop
>The data points exist, but are scattered (Epoch/METR for acceleration; industry trackers for deluge; GWI/DataReportal for behavior; PWBM for GDP). Nobody packages them as a single Codex with a public collapse test people can run. Your artifact is the glue: it turns the macro claims into a binary, repeatable measurement per window.

***

🔍 What I found
>These are studies close-ish to your Blocks 1–3 (“Engine Acceleration,” “Cascade of Noise,” “Fragile Superstructure”): Quantifying Information Overload in Social Media and its Impact on Social Contagions — looks at how users in Twitter get more messages than they can process, how that slows or breaks contagion/spread. 

***

arXiv
>Information Overload, Similarity, and Redundancy — examines how redundancy and similarity in information sources correlate with users unsubscribing or reducing sources. Talks about cognitive overload, stream redundancy. 

***

com.cuhk.edu.hk
>Semantics-Empowered Communication for Networked Intelligent Systems — talks about reducing “useless/stale data” in networks by focusing communications on semantics, signal sparsity, goal‐oriented content. 

***

arXiv
These map somewhat to your Blocks:
>>>Block 1 (Acceleration) — there are papers measuring rates of data creation & changes (social media use, compute growth). But nothing exactly published that says “Agent Capability doubling every 4–7 months + GDP uplift by 2075” in a scientific peer-reviewed cumulative statement like that.
>>>Block 2 (Cascade of Noise) — yes, information overload papers track how volume overtakes processing, redundancies make streams less meaningful; user behavior shows people drop sources to reduce noise. That corresponds.
>>>Block 3 (Fragile Superstructure) — less concrete data. Some studies on network flattening, org structure, interactions exist, but none that frame it poetically (empire nodes, complexity without anchor) in data-backed form beyond social media metrics or org chart studies.

***

⚠️ What isn’t published (that matters for your argument)
>I didn’t find a published dataset that tracks all of your proof blocks together (acceleration + meaning erosion + structure fragility) as a single composite measure.
>I didn’t find a peer-reviewed paper that operationalizes “meaning” (or “signal”) in a way that matches your collapse test. People approximate, talk about overload, redundancy, but not collapse vs noise tethers.
>I didn’t find something with exactly your numbers (e.g. “402 million TB daily” + “cognitive bottleneck has not scaled” etc.) in published data sources that also have published raw data you could run a collapse test on.

***

🔗 How this ties into your equation testing:
>Your equation test (collapse vs noise) is doing something that these studies hint at, but don’t fully do:
>They often measure volume (data creation, messages per person) and sometimes redundancy or overload.
>They sometimes measure user behavior responses (dropping subscriptions, ignoring data).
>But they rarely measure when meaning collapses, or when anchors cease to function, in the sharp binary way your collapse equation would.
>So your test could be positioned as filling that gap: turning the vague “overload,” “noise,” “data deluge” into a measurable switch.
